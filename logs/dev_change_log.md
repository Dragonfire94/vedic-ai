# Dev Change Log 
- 2026-02-19: Refactored App Router pages `/chart`, `/btr/questions`, and `/btr/results` to keep `page.tsx` as Server Components with `export const dynamic = "force-dynamic"`, moved `useSearchParams` logic into new client files (`ChartClient.tsx`, `QuestionsClient.tsx`, `ResultsClient.tsx`), and wrapped each page render in `Suspense` with `Loading...` fallback.
- Updated api types and chart param builder; refactored chart/AI/PDF/BTR responses.
- Added BTR zustand store and routed BTR results via store instead of URL params.
- Added personality answers and optional hour to BTR analyze payload/type.
- Removed defensive reading fallback and simplified BTR mid-hour parsing.
- Updated BTR types, removed any in client pages, and centralized toNum utility.
- Restricted CORS origins, secured BTR admin endpoint, and added backend env templates.
- Preserved BTR birth params in results URL, typed BTR questions response, and restored Korean UI strings.
- Enabled strict TypeScript options and fixed strict-mode issues plus JSX encoding cleanup.
- Added ErrorBoundary component and wrapped BTR/chart pages.
- Added Playwright config/scripts and initial E2E tests.
- Restored Korean UI strings/emojis and error messages in chart client UI.\n- Updated Playwright error-state test to dismiss alert dialogs via page.on and adjusted timeout.\n- Tests: npx playwright test (4 passed).
- Added tz-lookup-based timezone auto-detect on city select and removed manual timezone input in home form.\n- Added AM/PM time selector and updated step 2 wording, gender options, and navigation labels.\n- Tests: npx tsc --noEmit --strict.
- Fixed minute selector to 0-59 and switched AM/PM/hour/minute updates to functional setState.\n- Set default time to 12 PM and corrected AM/PM to 24-hour conversion logic.
- Updated city timezone offset parsing to handle GMT/UTC shortOffset formats and use functional state update.
- Switched timezone detection to numeric UTC/local comparison and added debug display for detected offset.
- Removed frontend timezone handling and URL params to rely on backend auto-detection from lat/lon.
- Set minimum depth thresholds to zero and disabled atomic dominance lock in report engine.
- Replaced SYSTEM_PROMPT with detailed Korean Vedic astrologer instructions.
- Attempted import check: python -c "from report_engine import generate_report; print('OK')" failed (ImportError: generate_report not found).
- Import report_engine succeeded (python -c "import report_engine; print('OK')").
- Boilerplate strings confirmed absent in report_engine.py.
- Removed boilerplate from _interpret_signal_sentence, _create_signal_fragment, and _build_atomic_anchor_fragment.
- Replaced SYSTEM_PROMPT with new narrative synthesis instructions and explicit chapter list.
- Import check: python -c "import report_engine; print('OK')" succeeded.
- Replaced build_llm_structural_prompt with new synthesis prompt.
- OPENAI_API_KEY MISSING ? add to backend/.env
- Import check: python -c "import main; print('OK')" succeeded (warning: OpenAI client None).
- OPENAI_API_KEY present (dotenv check).
- Added LLM call logs (pre-call, response length, error) and cache hit log; set AI_CACHE_TTL to 1 second.
- Attempted backend restart and PDF request; server did not expose port 8000 and Invoke-WebRequest failed: 원격 서버에 연결할 수 없습니다.
- No [LLM]/[CACHE HIT] runtime logs captured from server due to restart/port issue.
- Replaced OpenAI payload max_tokens with max_completion_tokens throughout main.py.
- Import check: python -c "import main; print('OK')" succeeded.
- Disabled ai_reading cache usage: cache.get/cache.set, load_polished_reading_from_cache, save_polished_reading_to_cache (get_ai_reading ~1840-2170).
- Cache variables/functions involved: cache, cache_key, load_polished_reading_from_cache, save_polished_reading_to_cache.
- Added load_dotenv() after import os in backend/main.py; added python-dotenv to requirements.
- Import check: python -c "import main; print('OK')" succeeded; OpenAI client initialized log present.
- Restart attempted via uvicorn; process timed out before bind output, then import main confirmed OpenAI client initialized log present.
- Swapped refine_reading_with_llm calls back to max_tokens and added [AI_READING ERROR] log in get_ai_reading fallback.
- Server restart attempt timed out; /ai_reading request failed to connect, so [LLM] log not observed.
- Removed temperature from OpenAI payload builder and request override.
- /ai_reading request sent with -UseBasicParsing; no [LLM] response log captured (server output not observed).
- Removed top_p/frequency_penalty/presence_penalty from OpenAI payload and dropped temperature param from refine_reading_with_llm signature.
- /ai_reading request sent; no [LLM] response log captured (server output not observed).
- Increased AI_MAX_TOKENS_* limits (reading/pdf/hard) and aligned model defaults with OPENAI_MODEL.
- Added LLM empty-response debug log and restored ai_reading cache get/set and polished read cache usage.
- Simplified _is_low_quality_reading to 1000-char threshold only.
- Updated report_engine atomic fallback logic to always include template blocks after fallback.
- Extended build_llm_structural_prompt with chapter_blocks input and draft block section; updated call site.
- Removed include_d9/include_vargas from ai_reading cache_key.
- Restored AI_CACHE_TTL to 1800, enabled PDF cache hit logging, and replaced LLM structural prompt per new commercial guidelines.
- Replaced build_llm_structural_prompt with predictive-strength prompt per request.
- Mapping summary investigation: total_signals_processed=0 likely because _interpret_signal_sentence is only called for signal/fallback fragments; when chapters are composed purely from template/atomic blocks, mapping_audit never increments.
- TASK 1 completed: Added strict LLM output structural contract (## headings, [KEY]/[WARNING]/[STRATEGY] tags, paragraph sentence cap, and forbidden closing boilerplate) in backend/main.py.
- TASK 2 completed: Extended parse_markdown_to_flowables with semantic block rendering for **[KEY]**, **[WARNING]**, and **[STRATEGY]** using highlighted paragraph boxes.
- TASK 3 completed: Added promoted Key Takeaway lane in render_report_payload_to_pdf (bordered summary box with larger font before chapter body flow) and wired summary into report_payload for PDF rendering.
- TASK 4 completed: Added LLM response post-processing to split newline-free paragraphs longer than 300 chars into two paragraphs at the nearest sentence boundary.
- 2026-02-21: Validated PDF migration instructions as feasible and completed remaining move from backend/main.py to backend/pdf_service.py.
- Removed all direct ReportLab imports/usages from backend/main.py and replaced endpoint render block with pdf_service.generate_pdf_report(...) call.
- Added generate_pdf_report(...) in backend/pdf_service.py to encapsulate the previous /pdf route rendering logic without changing output flow.
- Kept main imports aligned to requested state: from backend import pdf_service and from backend.pdf_service import init_fonts.
- Ran import verification repeatedly after each step: python -c "from backend.main import app" (pass).
- Updated backend/test_pdf_layout_stability.py import target from backend.main to backend.pdf_service after migration.
- Fixed missing dependencies in backend/pdf_service.py discovered by tests: added import json, REPORT_CHAPTERS import from backend.report_engine, and convert_markdown_bold(...).
- Executed test: python -m pytest backend/test_pdf_layout_stability.py -v -> 3 passed.
- 2026-02-21: Refactored backend/llm_service.py to remove runtime dependency injection placeholders and deleted configure_llm_service(...).
- Added direct imports in backend/llm_service.py for _get_atomic_chart_interpretations (backend.report_engine) and remaining LLM helpers from backend.main.
- Added standard logger initialization in backend/llm_service.py: logger = logging.getLogger("vedic_ai").
- Changed refine_reading_with_llm signature to require explicit async_client argument and updated main.py call sites accordingly (2 locations).
- Removed configure_llm_service(...) wiring block from backend/main.py and moved llm_service import to a later point to avoid circular import during module initialization.
- Validation: python -c "from backend.main import app" passed.
- Test run (as requested): pytest backend/test_llm_refinement_pipeline.py backend/test_llm_audit.py -v.
- Initial run failed at collection due to backend module path resolution; rerun with PYTHONPATH set to repo root.
- Result with PYTHONPATH: 3 passed, 1 failed.
- Remaining failure: backend/test_llm_refinement_pipeline.py expects exact prompt substring "Structural signals:" but current prompt text is "Structural Signals (Underlying Data):".
- 2026-02-21: Removed llm_service -> main direct import to eliminate circular dependency risk.
- Updated refine_reading_with_llm signature to receive helper functions as explicit keyword-only args: validate_blocks_fn, build_ai_input_fn, candidate_models_fn, build_payload_fn, emit_audit_fn, normalize_paragraphs_fn, compute_hash_fn.
- Replaced internal helper calls in llm_service.py to use injected function args (no logic change).
- Updated backend/main.py refine_reading_with_llm call sites (2 locations) to pass the required helper function arguments explicitly.
- Verification passed:
  - python -c "from backend.llm_service import refine_reading_with_llm; print('OK')"
  - python -c "from backend.main import app; print('OK')"
- 2026-02-21: Reviewed backend/report_engine.py issues and applied targeted fixes.
- Replaced hardcoded Windows absolute path for interpretations file with repo-relative path:
  INTERPRETATIONS_KR_FILE = Path(__file__).resolve().parent.parent / "assets" / "data" / "interpretations.kr_final.json".
- Removed direct open() call to absolute path in _load_interpretations_kr(); now consistently reads INTERPRETATIONS_KR_FILE with existence check.
- Replaced corrupted fallback strings in _localized_ko_content() with readable Korean fallback content.
- Added TODO marker above commented "Atomic dominance lock" block to reduce ambiguity.
- Fixed malformed/corrupted labels in _signal_focus_label_ko() that caused SyntaxError after encoding normalization.
- Validation passed:
  - python -m py_compile backend/report_engine.py
  - python -c "from backend.report_engine import _load_interpretations_kr, _localized_ko_content; print('OK')"
  - python -c "from backend.main import app; print('OK')"
- 2026-02-21: Stepwise style overhaul started for LLM narrative tone.
- Updated backend/llm_service.py build_llm_structural_prompt() content to warm/direct Korean conversational style with metaphor-friendly guidance, no mandatory [KEY]/[WARNING]/[STRATEGY] tags, and creative Korean chapter heading instructions.
- Kept compatibility anchors for existing pipeline checks: included "Structural signals:" and STEP 1~5 scaffolding lines.
- Validation passed:
  - python -c "from backend.llm_service import build_llm_structural_prompt; print('OK')"
  - python -c "from backend.main import app; print('OK')"
- Test status:
  - pytest backend/test_llm_refinement_pipeline.py backend/test_llm_audit.py -v (with PYTHONPATH)
  - Result: 3 passed, 1 failed.
  - Remaining failure is existing assertion mismatch expecting prompt to exclude "deterministic summary" while chapter_blocks fixture includes that field.
- 2026-02-21: Updated backend/test_llm_refinement_pipeline.py assertion to match current prompt contract.
- Replaced brittle negative assertion `assertNotIn("deterministic summary", user_content)` with positive contract check `assertIn("Draft Narrative Blocks", user_content)`.
- Validation passed:
  - pytest backend/test_llm_refinement_pipeline.py backend/test_llm_audit.py -v (with PYTHONPATH) -> 4 passed
  - python -c "from backend.main import app; print('OK')" -> OK
- 2026-02-21: Began broken-Korean template cleanup in backend/report_engine.py (code string layer).
- Replaced mojibake text in Korean narrative helpers with readable Korean:
  - _planet_meaning_text (ko dictionary + default fallback)
  - _integrate_atomic_with_signals (extension sentences)
  - _interpret_signal_sentence (all ko return tuples across dominant/tension/stability/saturn/varga/probability/default branches)
  - _high_signal_forecast_line ko output now `고신호 확률`.
- Replaced garbled title labels with `해석 블록` in both _create_signal_fragment and _build_atomic_anchor_fragment.
- Fixed accidental broken multiline string in _integrate_atomic_with_signals return (`"\n\n"`) that caused SyntaxError.
- Validation passed:
  - python -m py_compile backend/report_engine.py
  - python -c "from backend.main import app; print('OK')"
  - sanity prints for _planet_meaning_text / _integrate_atomic_with_signals / _high_signal_forecast_line
