# Dev Change Log 
- 2026-02-19: Refactored App Router pages `/chart`, `/btr/questions`, and `/btr/results` to keep `page.tsx` as Server Components with `export const dynamic = "force-dynamic"`, moved `useSearchParams` logic into new client files (`ChartClient.tsx`, `QuestionsClient.tsx`, `ResultsClient.tsx`), and wrapped each page render in `Suspense` with `Loading...` fallback.
- Updated api types and chart param builder; refactored chart/AI/PDF/BTR responses.
- Added BTR zustand store and routed BTR results via store instead of URL params.
- Added personality answers and optional hour to BTR analyze payload/type.
- Removed defensive reading fallback and simplified BTR mid-hour parsing.
- Updated BTR types, removed any in client pages, and centralized toNum utility.
- Restricted CORS origins, secured BTR admin endpoint, and added backend env templates.
- Preserved BTR birth params in results URL, typed BTR questions response, and restored Korean UI strings.
- Enabled strict TypeScript options and fixed strict-mode issues plus JSX encoding cleanup.
- Added ErrorBoundary component and wrapped BTR/chart pages.
- Added Playwright config/scripts and initial E2E tests.
- Restored Korean UI strings/emojis and error messages in chart client UI.\n- Updated Playwright error-state test to dismiss alert dialogs via page.on and adjusted timeout.\n- Tests: npx playwright test (4 passed).
- Added tz-lookup-based timezone auto-detect on city select and removed manual timezone input in home form.\n- Added AM/PM time selector and updated step 2 wording, gender options, and navigation labels.\n- Tests: npx tsc --noEmit --strict.
- Fixed minute selector to 0-59 and switched AM/PM/hour/minute updates to functional setState.\n- Set default time to 12 PM and corrected AM/PM to 24-hour conversion logic.
- Updated city timezone offset parsing to handle GMT/UTC shortOffset formats and use functional state update.
- Switched timezone detection to numeric UTC/local comparison and added debug display for detected offset.
- Removed frontend timezone handling and URL params to rely on backend auto-detection from lat/lon.
- Set minimum depth thresholds to zero and disabled atomic dominance lock in report engine.
- Replaced SYSTEM_PROMPT with detailed Korean Vedic astrologer instructions.
- Attempted import check: python -c "from report_engine import generate_report; print('OK')" failed (ImportError: generate_report not found).
- Import report_engine succeeded (python -c "import report_engine; print('OK')").
- Boilerplate strings confirmed absent in report_engine.py.
- Removed boilerplate from _interpret_signal_sentence, _create_signal_fragment, and _build_atomic_anchor_fragment.
- Replaced SYSTEM_PROMPT with new narrative synthesis instructions and explicit chapter list.
- Import check: python -c "import report_engine; print('OK')" succeeded.
- Replaced build_llm_structural_prompt with new synthesis prompt.
- OPENAI_API_KEY MISSING ? add to backend/.env
- Import check: python -c "import main; print('OK')" succeeded (warning: OpenAI client None).
- OPENAI_API_KEY present (dotenv check).
- Added LLM call logs (pre-call, response length, error) and cache hit log; set AI_CACHE_TTL to 1 second.
- Attempted backend restart and PDF request; server did not expose port 8000 and Invoke-WebRequest failed: 원격 서버에 연결할 수 없습니다.
- No [LLM]/[CACHE HIT] runtime logs captured from server due to restart/port issue.
- Replaced OpenAI payload max_tokens with max_completion_tokens throughout main.py.
- Import check: python -c "import main; print('OK')" succeeded.
- Disabled ai_reading cache usage: cache.get/cache.set, load_polished_reading_from_cache, save_polished_reading_to_cache (get_ai_reading ~1840-2170).
- Cache variables/functions involved: cache, cache_key, load_polished_reading_from_cache, save_polished_reading_to_cache.
- Added load_dotenv() after import os in backend/main.py; added python-dotenv to requirements.
- Import check: python -c "import main; print('OK')" succeeded; OpenAI client initialized log present.
- Restart attempted via uvicorn; process timed out before bind output, then import main confirmed OpenAI client initialized log present.
- Swapped refine_reading_with_llm calls back to max_tokens and added [AI_READING ERROR] log in get_ai_reading fallback.
- Server restart attempt timed out; /ai_reading request failed to connect, so [LLM] log not observed.
- Removed temperature from OpenAI payload builder and request override.
- /ai_reading request sent with -UseBasicParsing; no [LLM] response log captured (server output not observed).
- Removed top_p/frequency_penalty/presence_penalty from OpenAI payload and dropped temperature param from refine_reading_with_llm signature.
- /ai_reading request sent; no [LLM] response log captured (server output not observed).
- Increased AI_MAX_TOKENS_* limits (reading/pdf/hard) and aligned model defaults with OPENAI_MODEL.
- Added LLM empty-response debug log and restored ai_reading cache get/set and polished read cache usage.
- Simplified _is_low_quality_reading to 1000-char threshold only.
- Updated report_engine atomic fallback logic to always include template blocks after fallback.
- Extended build_llm_structural_prompt with chapter_blocks input and draft block section; updated call site.
- Removed include_d9/include_vargas from ai_reading cache_key.
- Restored AI_CACHE_TTL to 1800, enabled PDF cache hit logging, and replaced LLM structural prompt per new commercial guidelines.
- Replaced build_llm_structural_prompt with predictive-strength prompt per request.
- Mapping summary investigation: total_signals_processed=0 likely because _interpret_signal_sentence is only called for signal/fallback fragments; when chapters are composed purely from template/atomic blocks, mapping_audit never increments.
