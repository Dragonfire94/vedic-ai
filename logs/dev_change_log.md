# Dev Change Log 
- 2026-02-19: Refactored App Router pages `/chart`, `/btr/questions`, and `/btr/results` to keep `page.tsx` as Server Components with `export const dynamic = "force-dynamic"`, moved `useSearchParams` logic into new client files (`ChartClient.tsx`, `QuestionsClient.tsx`, `ResultsClient.tsx`), and wrapped each page render in `Suspense` with `Loading...` fallback.
- Updated api types and chart param builder; refactored chart/AI/PDF/BTR responses.
- Added BTR zustand store and routed BTR results via store instead of URL params.
- Added personality answers and optional hour to BTR analyze payload/type.
- Removed defensive reading fallback and simplified BTR mid-hour parsing.
- Updated BTR types, removed any in client pages, and centralized toNum utility.
- Restricted CORS origins, secured BTR admin endpoint, and added backend env templates.
- Preserved BTR birth params in results URL, typed BTR questions response, and restored Korean UI strings.
- Enabled strict TypeScript options and fixed strict-mode issues plus JSX encoding cleanup.
- Added ErrorBoundary component and wrapped BTR/chart pages.
- Added Playwright config/scripts and initial E2E tests.
- Restored Korean UI strings/emojis and error messages in chart client UI.\n- Updated Playwright error-state test to dismiss alert dialogs via page.on and adjusted timeout.\n- Tests: npx playwright test (4 passed).
- Added tz-lookup-based timezone auto-detect on city select and removed manual timezone input in home form.\n- Added AM/PM time selector and updated step 2 wording, gender options, and navigation labels.\n- Tests: npx tsc --noEmit --strict.
- Fixed minute selector to 0-59 and switched AM/PM/hour/minute updates to functional setState.\n- Set default time to 12 PM and corrected AM/PM to 24-hour conversion logic.
- Updated city timezone offset parsing to handle GMT/UTC shortOffset formats and use functional state update.
- Switched timezone detection to numeric UTC/local comparison and added debug display for detected offset.
- Removed frontend timezone handling and URL params to rely on backend auto-detection from lat/lon.
- Set minimum depth thresholds to zero and disabled atomic dominance lock in report engine.
- Replaced SYSTEM_PROMPT with detailed Korean Vedic astrologer instructions.
- Attempted import check: python -c "from report_engine import generate_report; print('OK')" failed (ImportError: generate_report not found).
- Import report_engine succeeded (python -c "import report_engine; print('OK')").
- Boilerplate strings confirmed absent in report_engine.py.
- Removed boilerplate from _interpret_signal_sentence, _create_signal_fragment, and _build_atomic_anchor_fragment.
- Replaced SYSTEM_PROMPT with new narrative synthesis instructions and explicit chapter list.
- Import check: python -c "import report_engine; print('OK')" succeeded.
- Replaced build_llm_structural_prompt with new synthesis prompt.
- OPENAI_API_KEY MISSING ? add to backend/.env
- Import check: python -c "import main; print('OK')" succeeded (warning: OpenAI client None).
- OPENAI_API_KEY present (dotenv check).
- Added LLM call logs (pre-call, response length, error) and cache hit log; set AI_CACHE_TTL to 1 second.
- Attempted backend restart and PDF request; server did not expose port 8000 and Invoke-WebRequest failed: 원격 서버에 연결할 수 없습니다.
- No [LLM]/[CACHE HIT] runtime logs captured from server due to restart/port issue.
- Replaced OpenAI payload max_tokens with max_completion_tokens throughout main.py.
- Import check: python -c "import main; print('OK')" succeeded.
- Disabled ai_reading cache usage: cache.get/cache.set, load_polished_reading_from_cache, save_polished_reading_to_cache (get_ai_reading ~1840-2170).
- Cache variables/functions involved: cache, cache_key, load_polished_reading_from_cache, save_polished_reading_to_cache.
- Added load_dotenv() after import os in backend/main.py; added python-dotenv to requirements.
- Import check: python -c "import main; print('OK')" succeeded; OpenAI client initialized log present.
- Restart attempted via uvicorn; process timed out before bind output, then import main confirmed OpenAI client initialized log present.
- Swapped refine_reading_with_llm calls back to max_tokens and added [AI_READING ERROR] log in get_ai_reading fallback.
- Server restart attempt timed out; /ai_reading request failed to connect, so [LLM] log not observed.
- Removed temperature from OpenAI payload builder and request override.
- /ai_reading request sent with -UseBasicParsing; no [LLM] response log captured (server output not observed).
- Removed top_p/frequency_penalty/presence_penalty from OpenAI payload and dropped temperature param from refine_reading_with_llm signature.
- /ai_reading request sent; no [LLM] response log captured (server output not observed).
- Increased AI_MAX_TOKENS_* limits (reading/pdf/hard) and aligned model defaults with OPENAI_MODEL.
- Added LLM empty-response debug log and restored ai_reading cache get/set and polished read cache usage.
- Simplified _is_low_quality_reading to 1000-char threshold only.
- Updated report_engine atomic fallback logic to always include template blocks after fallback.
- Extended build_llm_structural_prompt with chapter_blocks input and draft block section; updated call site.
- Removed include_d9/include_vargas from ai_reading cache_key.
- Restored AI_CACHE_TTL to 1800, enabled PDF cache hit logging, and replaced LLM structural prompt per new commercial guidelines.
- Replaced build_llm_structural_prompt with predictive-strength prompt per request.
- Mapping summary investigation: total_signals_processed=0 likely because _interpret_signal_sentence is only called for signal/fallback fragments; when chapters are composed purely from template/atomic blocks, mapping_audit never increments.
- TASK 1 completed: Added strict LLM output structural contract (## headings, [KEY]/[WARNING]/[STRATEGY] tags, paragraph sentence cap, and forbidden closing boilerplate) in backend/main.py.
- TASK 2 completed: Extended parse_markdown_to_flowables with semantic block rendering for **[KEY]**, **[WARNING]**, and **[STRATEGY]** using highlighted paragraph boxes.
- TASK 3 completed: Added promoted Key Takeaway lane in render_report_payload_to_pdf (bordered summary box with larger font before chapter body flow) and wired summary into report_payload for PDF rendering.
- TASK 4 completed: Added LLM response post-processing to split newline-free paragraphs longer than 300 chars into two paragraphs at the nearest sentence boundary.
- 2026-02-21: Validated PDF migration instructions as feasible and completed remaining move from backend/main.py to backend/pdf_service.py.
- Removed all direct ReportLab imports/usages from backend/main.py and replaced endpoint render block with pdf_service.generate_pdf_report(...) call.
- Added generate_pdf_report(...) in backend/pdf_service.py to encapsulate the previous /pdf route rendering logic without changing output flow.
- Kept main imports aligned to requested state: from backend import pdf_service and from backend.pdf_service import init_fonts.
- Ran import verification repeatedly after each step: python -c "from backend.main import app" (pass).
- Updated backend/test_pdf_layout_stability.py import target from backend.main to backend.pdf_service after migration.
- Fixed missing dependencies in backend/pdf_service.py discovered by tests: added import json, REPORT_CHAPTERS import from backend.report_engine, and convert_markdown_bold(...).
- Executed test: python -m pytest backend/test_pdf_layout_stability.py -v -> 3 passed.
- 2026-02-21: Refactored backend/llm_service.py to remove runtime dependency injection placeholders and deleted configure_llm_service(...).
- Added direct imports in backend/llm_service.py for _get_atomic_chart_interpretations (backend.report_engine) and remaining LLM helpers from backend.main.
- Added standard logger initialization in backend/llm_service.py: logger = logging.getLogger("vedic_ai").
- Changed refine_reading_with_llm signature to require explicit async_client argument and updated main.py call sites accordingly (2 locations).
- Removed configure_llm_service(...) wiring block from backend/main.py and moved llm_service import to a later point to avoid circular import during module initialization.
- Validation: python -c "from backend.main import app" passed.
- Test run (as requested): pytest backend/test_llm_refinement_pipeline.py backend/test_llm_audit.py -v.
- Initial run failed at collection due to backend module path resolution; rerun with PYTHONPATH set to repo root.
- Result with PYTHONPATH: 3 passed, 1 failed.
- Remaining failure: backend/test_llm_refinement_pipeline.py expects exact prompt substring "Structural signals:" but current prompt text is "Structural Signals (Underlying Data):".
